# mesa-ads — мини-движок видеорекламы

`mesa-ads` — это небольшой HTTP-сервис на Go, который подбирает видеорекламу под контекст запроса, учитывает бюджеты кампаний и собирает базовую статистику по показам и кликам.  

---

## Возможности

- Подбор релевантного ролика по:
  - языку (`language`),
  - гео (`geo`),
  - категории контента (`category`),
  - интересам (`interests`),
  - плейсменту (`placement`).
- Поддержка кампаний с моделью оплаты:
  - **CPM** (списание при показе),
  - **CPC** (списание при клике).
- Учёт **дневного** и **общего** бюджета кампании.
- Фиксация событий:
  - **Impression** (показ),
  - **Click** (клик).
- Идемпотентная обработка кликов по токену (повторный клик не списывает бюджет повторно).
- Мини-статистика по кампаниям за период:
  - показы,
  - клики,
  - расход,
  - CTR (считается на клиенте как `clicks / impressions`).
- Поведение **no-fill** — если подходящего объявления нет, возвращается `204 No Content`.

---

## Как user stories закрываются сервисом

**1. Зритель**  
Получает релевантный ролик с учётом:

- языка, гео, категории, интересов и плейсмента;
- ограничения по бюджету кампаний (кампании с исчерпанным бюджетом в подбор не попадают);
- опционального frequency-capping по `userID` (см. раздел «Принятые упрощения» — пока не включён глобально).

Если ни один креатив не прошёл фильтры или бюджеты, сервис возвращает `204 No Content` — это и есть no-fill.

---

**2. Рекламодатель**

- Денежные суммы хранятся в **целочисленных минимальных единицах** (например, центы).
- Для **CPM**-кампаний списание денег происходит при записи события `Impression`.
- Для **CPC**-кампаний списание происходит при записи события `Click`.
- В обоих случаях:
  - списание идёт через транзакцию в БД,
  - учитываются `daily_budget` и `total_budget`,
  - при недостатке бюджета операция отклоняется, событие не пишется.

---

**3. Оператор / аналитик**

Через эндпоинт `/api/v1/stats/overview` можно получить:

- суммарные показы,
- суммарные клики,
- суммарный расход (в минимальных единицах, например, копейки),
- по одному `campaign_id` и/или за произвольный период времени.

На основе этого легко строятся CTR и базовая аналитика. Хотя, на эту роль можно было бы прикрутить и метрику.

---

## Архитектура

Проект следует **hexagonal / ports & adapters** подходу:

### Domain (`internal/core/domain`)

Чистые структуры без зависимостей от инфраструктуры:

- `Campaign` — кампания (лимиты, ставки, даты, статус).
- `Creative` — конкретный ролик (video URL, landing URL, duration, язык, категория, плейсмент).
- `Targeting` — настройки таргета кампании (языки, гео, категории, интересы, плейсменты).
- `Impression`, `Click` — события.
- `UserContext` — контекст входящего запроса: `userID`, язык, гео, категория, интересы, плейсмент.

### Ports (`internal/core/port`)

- `AdUseCase` — интерфейс бизнес-операций:
  - `RequestAd(ctx, UserContext) (CreativeWithToken, error)` — подбор рекламы и запись показа (для CPM).
  - `RegisterClick(ctx, token string) (RedirectURL, error)` — регистрация клика и списание бюджета (для CPC).
  - `GetStats(ctx, StatsReq) (StatsResp, error)` — агрегированная статистика.

- `AdRepository` — интерфейс доступа к хранилищу:
  - выбор кандидатов: `GetEligibleCreatives(ctx, UserContext) ([]CreativeCandidate, error)`,
  - атомарная запись показов/кликов с изменением бюджета:
    - `CreateImpressionAndDeductBudget(ctx, Impression) error`,
    - `CreateClickAndDeductBudget(ctx, Click) error`,
  - выбор статистики: `GetStats(ctx, StatsReq) (StatsResp, error)`.

### Use case слой (`internal/adapter/usecase`)

Реализация `AdUseCase`:

- строит список кандидатов по таргету и бюджету,
- считает **eCPM** для ранжирования:

  - CPM: `eCPM = bid_cpm`,
  - CPC: `eCPM = bid_cpc × CTR_estimate × 1000`, где `CTR_estimate` по умолчанию **1%**,

- выбирает креатив с максимальным eCPM,
- создаёт `Impression` и списывает CPM-бюджет (в транзакции),
- при клике регистрирует `Click` и списывает CPC-бюджет (в транзакции),
- обеспечивает идемпотентность кликов через уникальный токен (см. ниже).

### Inbound адаптер (HTTP, `internal/adapter/http`)

- роутинг на `chi`:
  - `POST /api/v1/ad/request` — запрос показа,
  - `GET  /api/v1/ad/click/{token}` — клик-редирект,
  - `GET  /api/v1/stats/overview` — статистика.
- конвертация HTTP-моделей в доменные и обратно,
- логирование ошибок и основных событий.

### Outbound адаптер (Postgres, `internal/adapter/postgres`)

- хранение кампаний, креативов, таргета, показов и кликов в PostgreSQL (через `pgxpool`),
- `GetEligibleCreatives` фильтрует кандидатов:
  - по статусу кампании,
  - по датам,
  - по дневному и общему бюджету,
  - по таргетингу (язык, гео, категория, интересы, плейсмент),
- `CreateImpressionAndDeductBudget` и `CreateClickAndDeductBudget` выполняют:
  - проверку `remaining_daily_budget` и `remaining_total_budget`,
  - обновление остатков бюджета,
  - вставку события в таблицу `impressions` или `clicks`.

Защита от гонок достигается за счёт того, что проверка и обновление остатков бюджета выполняются **внутри одной транзакции с SELECT FOR UPDATE** — конкурентные запросы не могут «перескочить» друг друга.

### Инфраструктура

- `internal/config` — загрузка конфигурации из env (например, через `caarlos0/env`).
- `internal/db` — построение пула `pgxpool`, прогон миграций через `golang-migrate`, сидирование тестовыми данными.
- `migrations/` — SQL-схема проекта (таблицы `campaigns`, `creatives`, `targetings`, `impressions`, `clicks`).

---

## Структура проекта

```text
mesa-ads/
├── cmd/
│   └── main.go               # Точка входа приложения
├── internal/
│   ├── core/
│   │   ├── domain/           # Доменные сущности
│   │   └── port/             # Интерфейсы портов и DTO
│   ├── adapter/
│   │   ├── http/             # HTTP-хендлеры, роутинг
│   │   ├── postgres/         # Реализация AdRepository для Postgres
│   │   └── usecase/          # Реализация бизнес-логики (AdUseCase)
│   ├── config/               # Агрегатор конфигов
│   │   └── configs/          # HTTP, Logger, PostgreSQL
│   └── db/                   # Подключение к Postgres, миграции, сидирование
├── migrations/               # SQL-миграции (встроены через embed)
├── docs/
│   ├── mesa-ads.json         # Postman-коллекция
│   └── .env                  # Пример файла окружения
├── Dockerfile                # Образ приложения
├── docker-compose.yml        # Локальный стенд: Postgres + сервис
├── Makefile                  # Сборка, тесты, линтер, генерация моков
├── go.mod / go.sum           # Модуль и зависимости
├── .golangci.yml             # Конфиг golangci-lint
└── .mockery.yml              # Конфиг генерации моков
````

---

## Алгоритм подбора рекламы

1. **Построение контекста пользователя**

   Из `POST /api/v1/ad/request` формируется `UserContext`:

  * `userID` — идентификатор пользователя (используется для связи событий и frequency-capping),
  * `language`, `geo`, `category`, `placement`,
  * `interests` — массив интересов.

2. **Фильтрация кампаний и креативов**

   Репозиторий выбирает `CreativeCandidate`:

  * кампания **активна**,
  * текущая дата попадает в диапазон кампании,
  * у кампании есть **остаток** дневного и общего бюджета,
  * таргетинг кампании совместим с контекстом:

    * язык/гео/категория/плейсмент совпадают,
    * интересы пересекаются (хотя бы один общий интерес).

3. **(Опционально) Frequency-capping**

   Происходит за счёт поля `userID` в контексте запроса: перед показом рекламы, программа смотрит, показывалась ли для этого `userID` эта реклама. Если она уже показалась 3 раза (захаркоженное значение для простоты), то программа попытается найти другого рекламодателя для этого пользователя.

4. **Ранжирование по eCPM**

   Для каждого `CreativeCandidate` вычисляется **eCPM**:

  * для CPM:
    `eCPM = cpm_bid`
  * для CPC:
    `eCPM = cpc_bid × CTR_estimate × 1000`

   где `CTR_estimate` — простая константа (по умолчанию 1%, т.е. `0.01`), что позволяет сопоставить CPC и CPM в одной шкале.

5. **Выбор победителя**

  * кандидаты сортируются по `eCPM` по убыванию,
  * выбирается креатив с максимальным `eCPM`,
  * если кандидатов нет — возвращается `204 No Content`.

6. **Создание показа и списание CPM**

   Для CPM-кампаний:

  * формируется структура `Impression` с `campaign_id`, `creative_id`, `user_id`, `token`, `cost`,
  * репозиторий в одной транзакции:

    * проверяет и обновляет `remaining_daily_budget` и `remaining_total_budget`,
    * вставляет запись в таблицу `impressions`.

7. **Генерация токена для клика**

   Для каждого ответа генерируется уникальный `token` (UUID, но можно использовать что-то покороче), который используется:

  * в поле `ClickURL` ответа `api/v1/ad/request`,
  * как идентификатор клика для идемпотентности.

---

## Учёт событий и списание бюджета

### CPM-кампании

* Списание происходит **при создании `Impression`**.
* Одна транзакция:

  1. Проверка, что остатков бюджета достаточно (`remaining_*_budget >= cost`).
  2. Обновление остатков.
  3. Вставка `Impression`.
* Если бюджет исчерпан — транзакция откатывается, показ не записывается.

### CPC-кампании

* Списание происходит **при регистрации клика** (`GET /api/v1/ad/click/{token}`).
* Для кликов используется уникальный токен `token`:

  * в таблице `clicks` есть уникальный индекс по `token`,
  * повторный запрос с тем же `token` приводит к попытке вставить дубликат и не списывает бюджет второй раз.
* В транзакции:

  1. Проверяется, что клика с таким `token` ещё не было.
  2. Проверяется достаточность бюджета.
  3. Списывается `cpc_bid` и создаётся `Click`.

### Денежные суммы

* Все суммы (бюджеты, ставки, стоимость событий) хранятся в **целочисленных минимальных единицах** (например, центы).
* Это убирает проблемы с плавающей точкой и упрощает расчёты.

---

## Модель данных

Основные таблицы (логически):

* `campaigns`:

  * `id`,
  * `name`,
  * `daily_budget`, `total_budget`,
  * `remaining_daily_budget`, `remaining_total_budget`,
  * `cpm_bid`, `cpc_bid`,
  * `start_date`, `end_date`,
  * `status` (active/paused/finished).

* `creatives`:

  * `id`,
  * `campaign_id`,
  * `video_url`,
  * `landing_url`,
  * `duration`,
  * `language`,
  * `category`,
  * `placement`.

* `targetings`:

  * `campaign_id`,
  * `language`,
  * `geo`
  * `category`
  * `categories`
  * `interests`
  * `placements`

* `impressions`:

  * `id`,
  * `token`,
  * `campaign_id`,
  * `creative_id`,
  * `user_id`,
  * `cost`,
  * `created_at`.

* `clicks`:

  * `id`,
  * `token`,
  * `campaign_id`,
  * `creative_id`,
  * `user_id`,
  * `cost`,
  * `created_at`.

---

## Переменные окружения

Конфигурация состоит из нескольких разделов.

### Общие

| Переменная | Тип    | Значение по умолчанию | Описание                                  |
|------------|--------|-----------------------|-------------------------------------------|
| `ENV`      | string | `prod`                | Название окружения (`prod`/`dev`/`local`) |

### Логирование (`LOG_`)

| Переменная   | Тип    | По умолчанию | Описание                                        |
|--------------|--------|--------------|-------------------------------------------------|
| `LOG_LEVEL`  | string | `info`       | Уровень логов: `debug`, `info`, `warn`, `error` |
| `LOG_FORMAT` | string | `text`       | Формат логов: `text` или `json`                 |

### HTTP-сервер (`HTTP_`)

| Переменная  | Тип    | По умолчанию | Описание              |
|-------------|--------|--------------|-----------------------|
| `HTTP_PORT` | uint16 | `8080`       | TCP-порт HTTP-сервера |

В `docker-compose` порт пробрасывается наружу как `8080:8080`.

### PostgreSQL (`PSQL_`)

| Переменная            | Тип  | По умолчанию                                                           | Описание                                            |
|-----------------------|------|------------------------------------------------------------------------|-----------------------------------------------------|
| `PSQL_ADDR`           | URL  | `postgres://postgres:password@localhost:5432/postgres?sslmode=disable` | Строка подключения к Postgres                       |
| `PSQL_RUN_MIGRATIONS` | bool | `false`                                                                | Запуск миграций при старте (`true`/`false`)         |
| `PSQL_RUN_SEED`       | bool | `false`                                                                | Заполнение демо-данными при старте (`true`/`false`) |

Пример `.env` лежит в `docs/.env`.

---

## Быстрый старт

### Вариант 1: Docker Compose

Требуется установленный Docker и docker-compose.

```bash
docker-compose up
```

Будет поднято:

* `postgres` на `localhost:5435`,
* `mesa-ads` на `localhost:8080`.

При старте приложение:

* подключается к Postgres по `PSQL_ADDR`,
* при `PSQL_RUN_MIGRATIONS=true` прогоняет миграции из `migrations/`,
* при `PSQL_RUN_SEED=true` заполняет БД демо-данными.

### Вариант 2: Локальный запуск без Docker

Для запуска нужно выполнить 2 следующих пункта:
1. Подними Postgres локально и создай БД (по умолчанию `postgres`).
2. Укажи адрес PostgreSQL в перменной окружения `PSQL_ADDR`

Далее можно запустить скрипт, который применит переменные окружения и запустит программу

```bash
export PSQL_ADDR=postgres://postgres:password@localhost:5432/postgres
export PSQL_RUN_MIGRATIONS=true
export PSQL_RUN_SEED=true
go run ./cmd
```

или через `make`:

```bash
make build
./mesa-ads
```

После этого HTTP-сервер будет доступен на `http://localhost:8080`.

---

## HTTP API

Все эндпоинты идут с префиксом `/api/v1`, что расширяет задание из условия (`/ad/*`, `/stats/*`), но логически соответствует ему.

### 1. Подбор рекламы — `POST /api/v1/ad/request`

**Запрос**

```json
{
  "userID": "123",
  "language": "ru",
  "geo": "Russia",
  "category": "music",
  "interests": ["gaming"],
  "placement": "pre-roll"
}
```

Поля:

* `userID` — идентификатор зрителя (используется для связи событий и потенциального frequency-capping),
* `language`, `geo`, `category`, `placement` — контекст просмотра,
* `interests` — список интересов пользователя.

**Ответы**

* `200 OK` — найден креатив:

  ```json
  {
    "CreativeID": 2,
    "Duration": 42,
    "VideoURL": "https://example.com/video/2.mp4",
    "ClickURL": "/api/v1/ad/click/fbee64a8-adab-447d-a3ea-79add27f8a86"
  }
  ```

  Где:

  * `CreativeID` — id креатива,
  * `Duration` — длительность ролика (секунды),
  * `VideoURL` — ссылка на видео,
  * `ClickURL` — относительный URL для учёта клика (нужно вызывать браузером или редиректом).

* `204 No Content` — подходящего объявления нет (по таргету или бюджету).

* `400 Bad Request` — некорректный JSON.

* `500 Internal Server Error` — внутренняя ошибка.

**Пример curl**

```bash
curl -X POST "http://localhost:8080/api/v1/ad/request" \
  -H "Content-Type: application/json" \
  -d '{
    "userID": "123",
    "language": "ru",
    "geo": "Russia",
    "category": "music",
    "interests": ["gaming"],
    "placement": "pre-roll"
  }'
```

---

### 2. Клик по объявлению — `GET /api/v1/ad/click/{token}`

* `token` — токен, выданный в `ClickURL` из `/api/v1/ad/request`.

**Поведение:**

* при валидном токене:

  * записывается событие `Click`,
  * для CPC-кампании списывается бюджет,
  * выполняется `302 Found` редирект на `landing_url` креатива;
* при неизвестном токене:

  * `404 Not Found`;
* при внутренних ошибках:

  * ошибка логируется,
  * наружу отдаётся `404`.

Идемпотентность:

* повторный запрос с тем же `token` **не создаёт второй клик** и **не списывает бюджет повторно**.

**Пример curl**

```bash
curl -v "http://localhost:8080/api/v1/ad/click/fbee64a8-adab-447d-a3ea-79add27f8a86" -L
```

---

### 3. Статистика — `GET /api/v1/stats/overview`

Параметры query-строки:

* `from` — начало периода в RFC3339 (`2025-01-01T00:00:00Z`),
* `to` — конец периода в RFC3339,
* `campaign_id` — фильтр по кампании (int).

* Все параметры являются необязательными 
* Если `from`/`to` не заданы — берётся последний день (`now() - 24h .. now()`).

**Ответ**

```json
{
  "impressions": 1234,
  "clicks": 56,
  "cost": 78900
}
```

Где:

* `impressions` — количество показов,
* `clicks` — количество кликов,
* `cost` — суммарный расход в минимальных денежных единицах (например, копейки).

CTR можно посчитать на клиенте как `clicks / impressions`.

**Пример curl**

```bash
curl "http://localhost:8080/api/v1/stats/overview?campaign_id=1&from=2025-01-01T00:00:00Z&to=2025-01-02T00:00:00Z"
```

### Postman-коллекция

Готовую коллекцию запросов для тестирования API можно импортировать из файла:

* `docs/mesa-ads.json`

---

## Тесты

Запуск тестов:

```bash
make test
```

Покрыто:

* Unit-тестами:

  * ранжирование креативов по eCPM,
  * фильтрация по таргетингу и бюджетам,
  * списание бюджета для CPM и CPC.

---

## Логирование и наблюдаемость

* Логи структурированы и содержат:

  * уровень (`debug`/`info`/`warn`/`error`),
  * метки запроса (метод, путь),
  * идентификаторы кампании/креатива, токен клика, userID — там, где это критично,
  * текст ошибки (если есть).
* Формат задаётся через `LOG_FORMAT` (`text` или `json`).
* Логируются:

  * ошибки БД,
  * некорректные запросы,
  * ошибки списания бюджета.

---

## Принятые упрощения и TODO

Чтобы уложиться в формат тестового задания, сделаны следующие упрощения:

* **Простая оценка CTR**:
  * используется константа `CTR_estimate = 1%`,
* **Frequency-capping**:
  * используется константа: клиент может увидеть одну и ту же рекламу максимум 3 раза
* **Упрощённый таргетинг**:
  * все поля таргета рассматриваются как обязательные соответствия 
  * нет приоритезации типов таргета
* **Одна валюта и одно время**:
  * все расчёты ведутся в одной валюте,
  * используется единая временная зона (`time.Now().UTC()` / конфигурируемое TZ).
* **Один сервис**:
  * нет отдельного сервиса для статистики или асинхронной обработки событий,
  * статистика считается по данным той же БД.
* **И другие**:
  * Дневные лимиты рекламодателей не обновляются
  * Усложнённые SQL запросы

---

## Как бы сделал в проде

Если разворачивать такую систему в проде, важны:

1. **Хранение и аналитика**
  * основные данные кампаний и бюджетов — в PostgreSQL,
  * события (impressions/clicks) — в колоночном хранилище ClickHouse с партиционированием по времени и кампаниям,
  * ретеншн и TTL для событий.

2. **Счётчики бюджета**
  * горячие счётчики бюджетов в Redis с периодической синхронизацией с БД,
  * защита от overspend за счёт атомарных операций инкремента/декремента в кеше.

3. **Асинхронная обработка событий**
  * запись событий через очередь Kafka,
  * отдельный воркер для записи в хранилище и пересчёта агрегатов.

4. **Наблюдаемость**
  * метрики (Prometheus + Grafana),
  * распределённый трейсинг (OpenTelemetry),
  * алерты по:
    * росту http 5xx,
    * падению CTR,
    * аномальному расходу бюджета.

5. **Развёртывание**
  * контейнеризация Docker,
  * оркестрация Kubernetes,
  * `readiness` / `liveness` probes,
  * rolling деплой

---

## Ответы на вопросы из задания

### 1. Сталкивался ли ранее с задачами AdTech?

К сожалению нет. Мне понравилось заниматься составлением алгоритма предоставления подходящей клиену рекламы: это заставляет напрячь мозги и подумать о корнер кейсах

---

### 2. В чём основная сложность такого сервиса?

* Корректное **списание бюджетов** под высокой нагрузкой:
  * защита от гонок,
  * отсутствие двойных списаний,
  * соблюдение лимитов (daily/total).
* **Баланс между доходом и качеством**:
  * нужно одновременно учитывать eCPM и таргетинг,
  * не показывать нерелевантные объявления ради высокой ставки.
* **Масштабирование**:
  * разделение горячего пути (подбор креатива) и холодного (аналитика, отчёты),
  * минимизация обращений в БД на каждый запрос.

---

### 3. В чём лично для тебя была самая большая сложность?

Честно говоря - уложить всё в разумный объём за ограниченное время. Если бы от меня требовался продукт без поблажек по требованиям - я бы запросил не менее месяца на такое тз.
В техническом плане сложно было лишь определиться со способом подбора рекламы пользователю: есть много вариантов, но нужно выбрать самый жизнеспособный и простой.
С маленькой нагрузкой сложностей не много, а вот если RPS будет расти - нужно будет думать

---

### 4. Как масштабировать на 10k RPS, 100k RPS, 1M RPS?

Кратко:

* **10k RPS**
  * несколько статeless-инстансов приложения за load balancer’ом
  * индексы и грамотные запросы в PostgreSQL

* **100k RPS**
  * отделение горячего пути:
    * подбор объявлений по in-memory кэшу + периодические обновления из БД,
    * события уходят в очередь Kafka, асинхронно пишутся в хранилище;
  * шардирование БД по campaign_id;
  * вынос событий clicks, impressions в отдельное хранилище ClickHouse.

* **1M RPS**
  * хранение всех необходимых данных для обработки запросов в in-memory кэше
  * события пишутся в Kafka, а консьюмеры пишут их в СlickHouse
  * для статистики отдельная система OLAP с предагрегированными данными

---

### 5. Какие технологии нужно/требуется использовать в проде?

* Хранение:
  * PostgreSQL для конфигурации кампаний и бюджетов,
  * ClickHouse для событий и аналитики,
  * Redis для кэша и подсчёта бюджета.
* Транспорт:
  * HTTP/gRPC,
  * очередь сообщений Kafka для событий.
* Инфраструктура:
  * Docker + Kubernetes,
* Наблюдаемость:
  * Prometheus + Grafana,
  * OpenTelemetry (traces + logs + metrics),
  * централизованный лог-сторан (ELK/Loki).
* Безопасность:
  * rate-limiting, circuit breaker,
  * mTLS между сервисами,
